{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf188010",
   "metadata": {},
   "source": [
    "### Hourly Specific Features\n",
    "\n",
    "1. **Hour of the Day**: Differentiate between peak hours (like morning and evening commute times) and off-peak hours.\n",
    "\n",
    "2. **Weather Conditions**: \n",
    "   - **Temperature**: Hourly temperature variations.\n",
    "   - **Precipitation Intensity**: Sudden rain or snow can deter riders.\n",
    "   - **Wind Speed and Direction**: Can affect riding comfort and safety.\n",
    "\n",
    "3. **Air Quality Index**: Can change hourly, affecting outdoor activity levels.\n",
    "\n",
    "4. **Traffic Congestion**: Real-time traffic data to indicate road conditions.\n",
    "\n",
    "5. **Public Transport Usage**: Fluctuations in public transport ridership could correlate with bike usage.\n",
    "\n",
    "6. **Local Events**: Events happening in or around the city at specific hours that could increase or decrease bike usage.\n",
    "\n",
    "### Daily Specific Features\n",
    "\n",
    "1. **Day of the Week**: Differentiating between weekdays and weekends.\n",
    "\n",
    "2. **Sunrise/Sunset Times**: Influences riding habits, especially for leisure riders.\n",
    "\n",
    "3. **Special Daily Events**: Local daily events, like markets or sports games.\n",
    "\n",
    "4. **School and Work Schedules**: School days vs. holidays, typical workday patterns.\n",
    "\n",
    "5. **Day-Specific Promotions**: Any bike-share or cycling promotions specific to certain days.\n",
    "\n",
    "6. **Public Transport Schedules**: Variation in public transport services (e.g., reduced service on weekends).\n",
    "\n",
    "### Weekly Specific Features\n",
    "\n",
    "1. **Week of the Year**: Capture seasonal trends and holiday periods.\n",
    "\n",
    "2. **Weekly Weather Forecast**: Anticipated weather conditions for the week.\n",
    "\n",
    "3. **City-Wide Weekly Events**: Larger events that span multiple days or occur weekly.\n",
    "\n",
    "4. **Cycling Trends and Campaigns**: Any week-long campaigns or trends promoting cycling.\n",
    "\n",
    "5. **Weekly Traffic Patterns**: Changes in road traffic conditions.\n",
    "\n",
    "6. **Tourist Influx**: If the city is a tourist destination, weekly variations in tourist numbers can affect bike rides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e14ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA \n",
    "\n",
    "# https://www.kaggle.com/code/abhishekmamidi/time-series-preprocessing-to-modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f8ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "# Strike\n",
    "# Public Transport Accessibility\n",
    "# Socio economics\n",
    "# Bike lanes/ Traffic \n",
    "# Residential vs Business area\n",
    "\n",
    "# OneHotEncode also cyclic date data\n",
    "# Outliers\n",
    "# Scaling (revert the weather data scaler, and apply RollingStandardScaler)\n",
    "# Check for general upward trend (stationary)\n",
    "# Denoising using fft_denoiser (should be done for every feature i guess)\n",
    "\n",
    "# Use previous data (there seems to be more available on open_source)\n",
    "\n",
    "# Split based on forecasting, not only random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64b6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import problem\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from jours_feries_france.compute import JoursFeries\n",
    "from jours_feries_france.compute import JoursFeries\n",
    "import datetime\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf19066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_denoiser(x, n_components, to_real=True):\n",
    "\n",
    "    n = len(x)\n",
    "    \n",
    "    # compute the fft\n",
    "    fft = np.fft.fft(x, n)\n",
    "    \n",
    "    # compute power spectrum density\n",
    "    # squared magnitud of each fft coefficient\n",
    "    PSD = fft * np.conj(fft) / n\n",
    "    \n",
    "    # keep high frequencies\n",
    "    _mask = PSD > n_components\n",
    "    fft = _mask * fft\n",
    "    \n",
    "    # inverse fourier transform\n",
    "    clean_data = np.fft.ifft(fft)\n",
    "    \n",
    "    if to_real:\n",
    "        clean_data = clean_data.real\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cde968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class RollingStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window, mode='rolling'):\n",
    "        self.window = window\n",
    "        self.mode = mode\n",
    "        \n",
    "        # to fill in code\n",
    "        self.pd_object = None\n",
    "        self.w_mean = None\n",
    "        self.w_std = None\n",
    "        self.__fitted__ = False\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"RollingStandardScaler(window={self.window}, mode={self.mode})\"\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.pd_object = getattr(X, self.mode)(self.window)\n",
    "        self.w_mean = self.pd_object.mean()\n",
    "        self.w_std = self.pd_object.std()\n",
    "        self.__fitted__ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        self._check_fitted()\n",
    "        \n",
    "        standardized = X.copy()\n",
    "        return (standardized - self.w_mean) / self.w_std\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        self._check_fitted()\n",
    "        \n",
    "        unstandardized = X.copy()\n",
    "        return  (unstandardized * self.w_std) + self.w_mean\n",
    "        \n",
    "        \n",
    "    def _check_fitted(self):\n",
    "        \"\"\" Checks if the algorithm is fitted. \"\"\"\n",
    "        if not self.__fitted__:\n",
    "            raise ValueError(\"Please, fit the algorithm first.\")\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec24880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_filter(data, mode='rolling', window=30, threshold=3):\n",
    "    \n",
    "    msg = f\"Type must be of pandas.Series but {type(data)} was passed.\"\n",
    "    assert isinstance(data, pd.Series), msg\n",
    "    \n",
    "    series = data.copy()\n",
    "    \n",
    "    # rolling/expanding objects\n",
    "    pd_object = getattr(series, mode)(window=window)\n",
    "    mean = pd_object.mean()\n",
    "    std = pd_object.std()\n",
    "    \n",
    "    upper_bound = mean + threshold * std\n",
    "    lower_bound = mean - threshold * std\n",
    "    \n",
    "    outliers = ~series.between(lower_bound, upper_bound)\n",
    "    # fill false positives with False\n",
    "    outliers.iloc[:window] = [False] * window\n",
    "    \n",
    "    series = series.to_frame()\n",
    "    series['outliers'] = np.array(outliers.astype('int').values)\n",
    "    series.columns = ['Close', 'Outliers']\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2478b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cyclic_time(X, col, n):\n",
    "        \n",
    "    unique_values = X[col].unique()\n",
    "    cos_unique = np.cos(unique_values / n * 2 * np.pi)\n",
    "    sin_unique = np.sin(unique_values / n * 2 * np.pi)\n",
    "        \n",
    "    return unique_values, cos_unique, sin_unique\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    \n",
    "    # Encode the date information from the DateOfDeparture columns\n",
    "    X.loc[:, \"year\"] = X[\"date\"].dt.year\n",
    "    X.loc[:, \"week\"] = X[\"date\"].dt.isocalendar().week.astype(np.int64)\n",
    "    X.loc[:, \"month\"] = X[\"date\"].dt.month\n",
    "    X.loc[:, \"day\"] = X[\"date\"].dt.day\n",
    "    X.loc[:, \"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X.loc[:, \"hour\"] = X[\"date\"].dt.hour\n",
    "    \n",
    "\n",
    "    # Create a binary column for workdays: Workday if it's a weekday (Mon-Fri) and not a holiday\n",
    "    X.loc[:, \"is_workday\"] = (X['weekday'] < 5).astype(int)\n",
    "    \n",
    "    \n",
    "    X[\"date\"] = X['date'].dt.date\n",
    "    \n",
    "    years = list(range(min(X['year']), max(X['year']) + 1))\n",
    "    \n",
    "    bank_holidays = []\n",
    "    for year in years: \n",
    "        bank_holidays += JoursFeries.for_year(year).values()\n",
    "        \n",
    "        \n",
    "    school_holidays = pd.read_csv('data/school_holidays.csv')\n",
    "    school_holidays['date'] = pd.to_datetime(school_holidays['date'])\n",
    "\n",
    "    school_holidays = school_holidays.loc[(school_holidays['date'].dt.date >= min(X['date'])) & (school_holidays['date'].dt.date <= max(X['date']))]\n",
    "    school_holidays = school_holidays.loc[school_holidays['vacances_zone_c'] == True].date.dt.date.values\n",
    "\n",
    "    X.loc[:, \"if_School_Holiday\"] = X['date'].isin(school_holidays).astype(int)\n",
    "    X.loc[:, \"is_Bank_Holiday\"] = X['date'].isin(bank_holidays).astype(int)\n",
    "\n",
    "    X.loc[:, \"is_Rush_Hour\"]= 0\n",
    "    X.loc[((X['hour'] >= 6) & (X['hour'] <= 8)) | ((X['hour'] >= 17) & (X['hour'] <= 20)), 'is_Rush_Hour'] = 1\n",
    "\n",
    "    \n",
    "    zipped = zip(['hour', 'weekday', 'day', 'week', 'month'], [24, 7, 31, 52, 12])\n",
    "\n",
    "    for col, n in zipped:\n",
    "        unique_values, cos_unique, sin_unique = cyclic_time(X, col, n)\n",
    "        X[f'{col}_cos'] = X[col].replace(unique_values, sin_unique)\n",
    "        X[f'{col}_sin'] = X[col].replace(unique_values, cos_unique)\n",
    "\n",
    "        \n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=['date', 'hour', 'weekday', 'day', 'week', 'month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423d29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _additional_features(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    \n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "    \n",
    "    \n",
    "    weather_data = scaled_weather_data.loc[(scaled_weather_data['datetime'] >= min(X.date)) & (scaled_weather_data['datetime'] <= max(X.date))]\n",
    "\n",
    "    merged_data = pd.merge(X, weather_data,  how='left', left_on=['date'], right_on=['datetime'], validate=\"m:1\")\n",
    "    merged_data = merged_data.drop(columns=['datetime'])\n",
    "    \n",
    "    merged_data = pd.merge(merged_data, velib_stats,  how='left', right_on=['site_id'], left_on=['site_id'])\n",
    "    \n",
    "    final_data = _encode_dates(merged_data)\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f13a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_weather_data = pd.read_csv('data/scaled_weather_data.csv')\n",
    "scaled_weather_data['datetime'] = pd.to_datetime(scaled_weather_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c395d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "velib_stats = pd.read_csv('data/velib_processed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3840d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(Path(\"data\") / \"train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1e4b42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>log_bike_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48321</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 02:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48324</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-01 03:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48327</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 04:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48330</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2020-09-01 15:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48333</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2020-09-01 18:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                counter_id              counter_name    site_id  \\\n",
       "48321  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48324  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48327  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48330  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48333  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "\n",
       "                  site_name  bike_count                date  \\\n",
       "48321  28 boulevard Diderot         0.0 2020-09-01 02:00:00   \n",
       "48324  28 boulevard Diderot         1.0 2020-09-01 03:00:00   \n",
       "48327  28 boulevard Diderot         0.0 2020-09-01 04:00:00   \n",
       "48330  28 boulevard Diderot         4.0 2020-09-01 15:00:00   \n",
       "48333  28 boulevard Diderot         9.0 2020-09-01 18:00:00   \n",
       "\n",
       "      counter_installation_date         coordinates counter_technical_id  \\\n",
       "48321                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48324                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48327                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48330                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48333                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "\n",
       "        latitude  longitude  log_bike_count  \n",
       "48321  48.846028   2.375429        0.000000  \n",
       "48324  48.846028   2.375429        0.693147  \n",
       "48327  48.846028   2.375429        0.000000  \n",
       "48330  48.846028   2.375429        1.609438  \n",
       "48333  48.846028   2.375429        2.302585  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e7aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca5fc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "def adf_test(timeseries):\n",
    "    print(\"Results of Dickey-Fuller Test:\")\n",
    "    dftest = adfuller(timeseries, autolag=\"AIC\")\n",
    "    dfoutput = pd.Series(\n",
    "        dftest[0:4],\n",
    "        index=[\n",
    "            \"Test Statistic\",\n",
    "            \"p-value\",\n",
    "            \"#Lags Used\",\n",
    "            \"Number of Observations Used\",\n",
    "        ],\n",
    "    )\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput[\"Critical Value (%s)\" % key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d6ab445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Dickey-Fuller Test:\n",
      "Test Statistic                  -157.112951\n",
      "p-value                            0.000000\n",
      "#Lags Used                         1.000000\n",
      "Number of Observations Used    49998.000000\n",
      "Critical Value (1%)               -3.430481\n",
      "Critical Value (5%)               -2.861598\n",
      "Critical Value (10%)              -2.566801\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# https://www.statsmodels.org/dev/examples/notebooks/generated/stationarity_detrending_adf_kpss.html\n",
    "stationary_test = data.set_index('date')['log_bike_count']\n",
    "adf_test(stationary_test.sample(n=50000, random_state=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0fb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cea15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec8e767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Outliers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53148</th>\n",
       "      <td>2.564949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57868</th>\n",
       "      <td>2.484907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57893</th>\n",
       "      <td>3.828641</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66181</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66273</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919731</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921014</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925239</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928156</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928465</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Close  Outliers\n",
       "53148   2.564949         1\n",
       "57868   2.484907         1\n",
       "57893   3.828641         1\n",
       "66181   0.693147         1\n",
       "66273   0.000000         1\n",
       "...          ...       ...\n",
       "919731  0.000000         1\n",
       "921014  0.693147         1\n",
       "925239  0.000000         1\n",
       "928156  0.000000         1\n",
       "928465  0.000000         1\n",
       "\n",
       "[235 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = basic_filter(data['log_bike_count'])\n",
    "outliers.loc[outliers['Outliers'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6e6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = _additional_features(data)\n",
    "\n",
    "transformed_data.drop(columns=['counter_id', 'site_id', 'bike_count', 'counter_installation_date', 'coordinates', 'counter_technical_id'], inplace=True)\n",
    "\n",
    "one_hot_encoded_data = pd.get_dummies(transformed_data, columns=['counter_name', 'site_name'], dtype=int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6a4687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:55:39] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 50\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m              'objective':['reg:squarederror'],\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m              'learning_rate': [.03, 0.05, .07], #so called `eta` value\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mprint(xgb_grid.best_score_)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mprint(xgb_grid.best_params_)\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m xgb1 \u001b[38;5;241m=\u001b[39m XGBRegressor(colsample_bytree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.07\u001b[39m, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m, min_child_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, nthread \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:linear\u001b[39m\u001b[38;5;124m'\u001b[39m, subsample \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mxgb1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y = one_hot_encoded_data['log_bike_count']\n",
    "X = one_hot_encoded_data.drop(columns=['log_bike_count'])\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 18)\n",
    "\n",
    "# Various hyper-parameters to tune\n",
    "xgb1 = XGBRegressor()\n",
    "\n",
    "\"\"\"parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "              \n",
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "        \n",
    "params = { 'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500, 1000],\n",
    "           'colsample_bytree': [0.3, 0.7]}\n",
    "           \n",
    "           \n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "xgb1 = XGBRegressor(colsample_bytree = 0.7, learning_rate = 0.07, max_depth = 7, min_child_weight = 4, n_estimators = 500, nthread = 4, objective = 'reg:linear', subsample = 0.7)\n",
    "xgb1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d18219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_train, xgb1.predict(X_train), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60590767",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_parquet(Path(\"data\") / \"test_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_transformed_data = _additional_features(final_test)\n",
    "final_transformed_data.drop(columns=['counter_id', 'site_id', 'counter_installation_date', 'coordinates', 'counter_technical_id'], inplace=True)\n",
    "final_one_hot_encoded_data = pd.get_dummies(final_transformed_data, columns=['counter_name', 'site_name'], dtype=int) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = xgb1.predict(final_one_hot_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = pd.DataFrame({'log_bike_count': final_predictions, 'Id': final_one_hot_encoded_data.index}, columns=['Id', 'log_bike_count'])\n",
    "final_submission.to_csv('final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba615d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7091e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeee7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, xgb1.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e6326",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#merged_data[merged_data.temp.isna()].isna().sum()\n",
    "\n",
    "# All the columns where name.isna() lack weather data. data set incomplete. (2021-03-28 02:00:00). Imputed from data before\n",
    "# merged_data.sort_values(by='date').loc[merged_data.name.isna()].date.unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5feb47c-c05f-442b-8009-4b7272bdf84f",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb001e-3446-492c-b887-6ed485448495",
   "metadata": {},
   "source": [
    "Let's now construct our first linear model with [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). We use a few helper functions defined in `problem.py` of the starting kit to load the public train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f99dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "data_transformer = FunctionTransformer(_additional_features, validate=False)\n",
    "data_transformer.fit_transform(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fa5fa-099f-4b98-b47c-98f4e11ea0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem\n",
    "\n",
    "X_train, y_train = problem.get_train_data()\n",
    "X_test, y_test = problem.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e579d41-9f7a-4b50-b07d-31866168a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea2f21-1d7b-48d0-87ce-e55440a99360",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c396114-de58-40b4-bfd3-10c0c7f4f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ebcf8-8769-46b4-93f8-d54a30194f87",
   "metadata": {},
   "source": [
    "Where `y` contains the `log_bike_count` variable. \n",
    "\n",
    "The test set is in the future as compared to the train set,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252bf7ea-2def-46e1-b4fd-583c0e2c3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Train: n_samples={X_train.shape[0]},  {X_train[\"date\"].min()} to {X_train[\"date\"].max()}'\n",
    ")\n",
    "print(\n",
    "    f'Test: n_samples={X_test.shape[0]},  {X_test[\"date\"].min()} to {X_test[\"date\"].max()}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10626d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(transformed_data[['counter_name', 'site_name']])\n",
    "onehot_encoded = enc.transform(transformed_data[['counter_name', 'site_name']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6907765",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoded = pd.get_dummies(transformed_data[['counter_name', 'site_name']], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25806d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.merge(transformed_data, onehot_encoded, left_index=True, right_index=True).drop(columns=['counter_name','site_name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca953d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e7450-5529-47e5-a311-c78e364048ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "data_transformer = FunctionTransformer(_additional_features)\n",
    "date_cols = ['year', 'hour_cos', 'hour_sin', 'weekday_cos', 'weekday_sin', 'day_cos', \n",
    "             'day_sin', 'week_cos', 'week_sin', 'month_cos', 'month_sin']\n",
    "\n",
    "categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "categorical_cols = [\"counter_name\", \"site_name\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"date\", OneHotEncoder(handle_unknown=\"ignore\"), date_cols),\n",
    "        (\"cat\", categorical_encoder, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "regressor = Ridge()\n",
    "\n",
    "pipe = make_pipeline(data_transformer, preprocessor, regressor)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e16fe-50fd-4229-af39-a6ebefad0a1e",
   "metadata": {},
   "source": [
    "We then evaluate this model with the RMSE metric,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d803c-80ea-4423-af83-5666c799838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\n",
    "    f\"Train set, RMSE={mean_squared_error(y_train, pipe.predict(X_train), squared=False):.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set, RMSE={mean_squared_error(y_test, pipe.predict(X_test), squared=False):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298cbed-ff41-49f4-9e51-6925410fc083",
   "metadata": {},
   "source": [
    "The model doesn't have enough capacity to generalize on the train set, since we have lots of data with relatively few parameters. However it happened to work somewhat better on the test set. We can compare these results with the baseline predicting the mean value,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e09d6-cbe8-4911-bfe1-ef5e939cacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline mean prediction.\")\n",
    "print(\n",
    "    f\"Train set, RMSE={mean_squared_error(y_train, np.full(y_train.shape, y_train.mean()), squared=False):.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set, RMSE={mean_squared_error(y_test, np.full(y_test.shape, y_test.mean()), squared=False):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753fe34-a94b-4d18-9497-e516c1ccc4e2",
   "metadata": {},
   "source": [
    "which illustrates that we are performing better than the baseline.\n",
    "\n",
    "Let's visualize the predictions for one of the stations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2509f6-7cba-4671-8363-d4abda33ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (X_test[\"counter_name\"] == \"Totem 73 boulevard de Sébastopol S-N\")\n",
    "    & (X_test[\"date\"] > pd.to_datetime(\"2021/09/01\"))\n",
    "    & (X_test[\"date\"] < pd.to_datetime(\"2021/09/02\"))\n",
    ")\n",
    "\n",
    "df_viz = X_test.loc[mask].copy()\n",
    "df_viz[\"bike_count\"] = np.exp(y_test[mask.values]) - 1\n",
    "df_viz[\"bike_count (predicted)\"] = np.exp(pipe.predict(X_test[mask])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab3196-90bd-4918-a509-01cd38b1adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "df_viz.plot(x=\"date\", y=\"bike_count\", ax=ax)\n",
    "df_viz.plot(x=\"date\", y=\"bike_count (predicted)\", ax=ax, ls=\"--\")\n",
    "ax.set_title(\"Predictions with Ridge\")\n",
    "ax.set_ylabel(\"bike_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e813e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# School Holidays\n",
    "#!pip install vacances-scolaires-france\n",
    "\n",
    "# Bank Holidays\n",
    "#!pip install jours-feries-france==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510ac0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27173dfa",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoded_data = pd.get_dummies(extract_final, columns = ['counter_name', 'site_name', 'preciptype'], dtype=int) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(one_hot_encoded_data)\n",
    "scaler.transform(one_hot_encoded_data)\n",
    "\n",
    "y = one_hot_encoded_data['log_bike_count']\n",
    "X = one_hot_encoded_data.drop(columns=['log_bike_count'])\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 18)\n",
    "\n",
    "# Various hyper-parameters to tune\n",
    "xgb1 = XGBRegressor()\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n",
    "\n",
    "xgb1 = XGBRegressor(colsample_bytree = 0.7, learning_rate = 0.07, max_depth = 7, min_child_weight = 4, n_estimators = 500, nthread = 4, objective = 'reg:linear', silent = 1, subsample = 0.7)\n",
    "xgb1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c745dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "regressor = Ridge()\n",
    "\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_train, regressor.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e517105",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, regressor.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_train, regressor.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96392d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
